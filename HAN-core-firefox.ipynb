{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import numpy as np\n",
    "import json, re, nltk, string\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Bidirectional,\n",
    "    BatchNormalization,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    RepeatVector,\n",
    "    TimeDistributed,\n",
    "    Permute,\n",
    "    multiply,\n",
    "    Lambda,\n",
    "    Activation,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam # - Works\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset\n",
    "mozilla_core\n",
    "\n",
    "mozilla_firefox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "all_bugs_json = './data/mozilla_core/deep_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GLOVE\n",
    "# glove_file = './data/mozilla_core/vectors.txt'\n",
    "# tmp_file = './data/mozilla_core/glove.txt'\n",
    "# glove2word2vec(glove_file, tmp_file)\n",
    "# wordvec_model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "# vocabulary = wordvec_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec parameters\n",
    "min_word_frequency = 5\n",
    "embed_size = 200\n",
    "context_window = 5\n",
    "\n",
    "# NN hyperparameters\n",
    "num_cv = 10\n",
    "max_sentence_num = 20\n",
    "max_sentence_len = 10\n",
    "num_rnn_unit = 512\n",
    "num_dense_unit = 1000\n",
    "rank_k = 10\n",
    "batch_size = 256\n",
    "\n",
    "# Mozilla repeated sentence\n",
    "removal_sent = ['Steps to reproduce:',\n",
    "                'Actual results:',\n",
    "                'Expected results:']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186173\n",
      "186173\n",
      "181308\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "with open(all_bugs_json) as data_file:\n",
    "    text = data_file.read()\n",
    "    text = text.replace('\" : NULL', '\" : \"NULL\"')\n",
    "    data = json.loads(text, strict=False)\n",
    "\n",
    "open_title = []\n",
    "open_desc = []\n",
    "closed_title = []\n",
    "closed_desc = []\n",
    "closed_owner = []\n",
    "for item in data:\n",
    "    # Mozilla\n",
    "    status = ['VERIFIED', 'RESOLVED', 'CLOSED']\n",
    "    invalid_owner = ['nobody@mozilla.org', 'nobody@bugzilla.org']\n",
    "    \n",
    "    closed_title.append(item['issue_title'])\n",
    "    closed_desc.append(item['description'])\n",
    "    closed_owner.append(item['owner'])\n",
    "  \n",
    "    open_title.append(item['issue_title'])\n",
    "    open_desc.append(item['description'])\n",
    "\n",
    "closed_title_20 = []\n",
    "closed_desc_20 = []\n",
    "closed_owner_20 = []\n",
    "owner = {}\n",
    "for key in closed_owner:\n",
    "    owner[key] = owner.get(key, 0) + 1\n",
    "for i in range(len(closed_owner)):\n",
    "    if owner[closed_owner[i]] >= 20:\n",
    "        closed_title_20.append(closed_title[i])\n",
    "        closed_desc_20.append(closed_desc[i])\n",
    "        closed_owner_20.append(closed_owner[i])\n",
    "\n",
    "print(len(open_title))\n",
    "print(len(closed_title))\n",
    "print(len(closed_title_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nobody@mozilla.org', 103107)\n",
      "('general@js.bugs', 5590)\n",
      "('attinasi@formerly-netscape.com.tld', 2574)\n",
      "('karnaze@formerly-netscape.com.tld', 2513)\n",
      "('rods@formerly-netscape.com.tld', 2342)\n",
      "('darin.moz@gmail.com', 1993)\n",
      "('jst@mozilla.org', 1615)\n",
      "('dbaron@dbaron.org', 1394)\n",
      "('joki@formerly-netscape.com.tld', 1306)\n",
      "('general@dom.bugs', 1191)\n",
      "359\n"
     ]
    }
   ],
   "source": [
    "# Owner details\n",
    "owner_cnt = {}\n",
    "for owner in closed_owner_20:\n",
    "    owner_cnt[owner] = owner_cnt.get(owner, 0) + 1\n",
    "sorted_owner_cnt = sorted(owner_cnt.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in range(10):\n",
    "    print(sorted_owner_cnt[i])\n",
    "print(len(sorted_owner_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess(title, desc):\n",
    "    # Remove \\r and repeated sentence\n",
    "    current_title = title.replace('\\r', ' ')\n",
    "    current_desc = desc.replace('\\r', ' ')\n",
    "    for sent in removal_sent:\n",
    "        current_desc = current_desc.replace(sent, ' ')\n",
    "    # Remove URLs\n",
    "    current_desc = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', current_desc)\n",
    "    # Change to lower case\n",
    "    current_title = current_title.lower()\n",
    "    current_desc = current_desc.lower()\n",
    "    # Remove stack trace\n",
    "    start_loc = current_desc.find(\"stack trace\")\n",
    "    current_desc = current_desc[:start_loc]    \n",
    "    # Remove hex code\n",
    "    current_title = re.sub(r'(\\w+)0x\\w+', '', current_title)\n",
    "    current_desc = re.sub(r'(\\w+)0x\\w+', '', current_desc)\n",
    "    # Tokenize sentence\n",
    "    current_title_tokens = nltk.sent_tokenize(current_title)\n",
    "    current_desc_tokens = nltk.sent_tokenize(current_desc)\n",
    "    current_desc_tokens_list = [desc.split('\\n') for desc in current_desc_tokens]\n",
    "    current_desc_tokens = []\n",
    "    for desc in current_desc_tokens_list:\n",
    "        current_desc_tokens += desc\n",
    "    # Remove punctuation\n",
    "    def remove_punct(report):\n",
    "        report_filter = []\n",
    "        for sent in report:\n",
    "            for punct in string.punctuation:\n",
    "                sent = sent.replace(punct, '')\n",
    "            report_filter.append(sent)\n",
    "        return report_filter\n",
    "    current_title_filter = remove_punct(current_title_tokens)\n",
    "    current_desc_filter = remove_punct(current_desc_tokens)\n",
    "    # Tokenize word\n",
    "    current_title_filter = [nltk.word_tokenize(sent) for sent in current_title_filter]\n",
    "    current_desc_filter = [nltk.word_tokenize(sent) for sent in current_desc_filter]\n",
    "    # Lemmatization\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "    tagged_title = [nltk.pos_tag(title) for title in current_title_filter]\n",
    "    tagged_desc = [nltk.pos_tag(desc) for desc in current_desc_filter]\n",
    "    current_title_lemm = [[WordNetLemmatizer().lemmatize(tag[0], pos=get_wordnet_pos(tag[1]) or wordnet.NOUN) for tag in title] for title in tagged_title]\n",
    "    current_desc_lemm = [[WordNetLemmatizer().lemmatize(tag[0], pos=get_wordnet_pos(tag[1]) or wordnet.NOUN) for tag in desc] for desc in tagged_desc]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    current_title_stop = [[word for word in title if not word in stop_words] for title in current_title_lemm]\n",
    "    current_desc_stop = [[word for word in desc if not word in stop_words] for desc in current_desc_lemm]\n",
    "    # Merge title and description\n",
    "    current_report = current_title_stop + current_desc_stop\n",
    "    current_report = list(filter(None, current_report))\n",
    "    \n",
    "    return current_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bug reports for pre-training word vectors\n",
    "open_report = []\n",
    "open_word = {}\n",
    "for i in range(len(open_title[:1000])):\n",
    "    current_report = preprocess(open_title[i], open_desc[i])\n",
    "    # Flatten\n",
    "    current_report = [word for sent in current_report for word in sent]\n",
    "    open_report.append(current_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word vectors\n",
    "wordvec_model = Word2Vec(open_report, min_count=min_word_frequency, vector_size=embed_size, window=context_window)\n",
    "vocabulary = wordvec_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bug reports for training and testing\n",
    "closed_report = []\n",
    "closed_owner = []\n",
    "for i in range(len(closed_title_20[0:1000])):\n",
    "    current_report = preprocess(closed_title_20[i], closed_desc_20[i])\n",
    "    closed_report.append(current_report)\n",
    "    closed_owner.append(closed_owner_20[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the words that is not present in the vocabulary\n",
    "update_report = []\n",
    "update_owner = []\n",
    "for i in range(len(closed_owner[0:1000])):\n",
    "    update_sents = []\n",
    "    for sent in closed_report[i]:\n",
    "        current_sent = [word for word in sent if word in vocabulary]\n",
    "        update_sents.append(current_sent)\n",
    "    update_sents = list(filter(None, update_sents))\n",
    "    update_report.append(update_sents)\n",
    "    update_owner.append(closed_owner[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words to numbers\n",
    "flatten_report = []\n",
    "for report in update_report:\n",
    "    for sent in report:\n",
    "        flatten_report.append(sent)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(flatten_report)\n",
    "\n",
    "for report in update_report:\n",
    "    for sent in report:\n",
    "        for i, word in enumerate(sent):\n",
    "            sent[i] = tokenizer.word_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = wordvec_model.wv[word]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define topk_accuracy\n",
    "def topk_accuracy(prediction, y_test, classes, rank_k=10):\n",
    "    accuracy = []\n",
    "    sortedIndices = []\n",
    "    pred_classes = []\n",
    "    for ll in prediction:\n",
    "        sortedIndices.append(\n",
    "            sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True)\n",
    "        )\n",
    "    for k in range(1, rank_k + 1):\n",
    "        id = 0\n",
    "        trueNum = 0\n",
    "        for sortedInd in sortedIndices:\n",
    "            pred_classes.append(classes[sortedInd[:k]])\n",
    "            if np.argmax(y_test[id]) in sortedInd[:k]:\n",
    "                trueNum += 1\n",
    "            id += 1\n",
    "        accuracy.append((float(trueNum) / len(prediction)) * 100)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def f_measure(prediction, y_test, classes, mode='macro'):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    sortedIndices = []   \n",
    "    for ll in prediction:\n",
    "        sortedIndices.append(\n",
    "            sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True)\n",
    "        )\n",
    "    id = 0\n",
    "    for sortedInd in sortedIndices:\n",
    "        ind = np.argmax(y_test[id])\n",
    "        if ind in sortedInd[:10]:\n",
    "            y_pred.append(ind)\n",
    "        else:\n",
    "            y_pred.append(-1)\n",
    "        id += 1\n",
    "    for y in y_test:\n",
    "        y_true.append(np.argmax(y))\n",
    "            \n",
    "    f1 = f1_score(y_true, y_pred, average = mode)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# class defining the custom attention layer\n",
    "class HierarchicalAttentionNetwork(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(HierarchicalAttentionNetwork, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim,)))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weigh = [self.W, self.b, self.u]\n",
    "        super(HierarchicalAttentionNetwork, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "\n",
    "        ait = K.exp(K.squeeze(K.dot(uit, self.u), -1))\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        weighted_input = x * K.expand_dims(ait)\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "    \n",
    "    def _get_attention_weights(self, X):\n",
    "\n",
    "        uit = K.tanh(K.bias_add(K.dot(X, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        return ait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepa\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV83, top1 - ... - top10 accuracy:  [9.523809523809524, 36.904761904761905, 39.285714285714285, 48.80952380952381, 50.0, 52.38095238095239, 65.47619047619048, 67.85714285714286, 67.85714285714286, 69.04761904761905]\n",
      "2\n",
      "CV83, top1 - ... - top10 accuracy:  [5.952380952380952, 33.33333333333333, 44.047619047619044, 50.0, 51.19047619047619, 53.57142857142857, 55.952380952380956, 64.28571428571429, 67.85714285714286, 72.61904761904762]\n",
      "3\n",
      "CV82, top1 - ... - top10 accuracy:  [18.072289156626507, 28.915662650602407, 37.34939759036144, 48.19277108433735, 54.21686746987952, 57.831325301204814, 60.24096385542169, 62.65060240963856, 63.85542168674698, 66.26506024096386]\n",
      "4\n",
      "CV82, top1 - ... - top10 accuracy:  [8.433734939759036, 21.686746987951807, 27.710843373493976, 32.53012048192771, 34.93975903614458, 42.168674698795186, 49.39759036144578, 59.036144578313255, 60.24096385542169, 72.28915662650603]\n",
      "5\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002EE6020B040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "CV85, top1 - ... - top10 accuracy:  [9.30232558139535, 13.953488372093023, 26.744186046511626, 36.04651162790697, 41.86046511627907, 44.18604651162791, 53.48837209302325, 62.7906976744186, 62.7906976744186, 68.6046511627907]\n",
      "6\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002EE4A4F3E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "CV84, top1 - ... - top10 accuracy:  [7.0588235294117645, 18.823529411764707, 24.705882352941178, 32.94117647058823, 40.0, 43.529411764705884, 52.94117647058824, 56.470588235294116, 60.0, 64.70588235294117]\n",
      "7\n",
      "CV85, top1 - ... - top10 accuracy:  [16.27906976744186, 23.25581395348837, 31.3953488372093, 40.69767441860465, 45.348837209302324, 51.162790697674424, 54.65116279069767, 55.81395348837209, 56.97674418604651, 61.627906976744185]\n",
      "8\n",
      "CV83, top1 - ... - top10 accuracy:  [7.142857142857142, 15.476190476190476, 26.190476190476193, 32.142857142857146, 32.142857142857146, 40.476190476190474, 41.66666666666667, 44.047619047619044, 47.61904761904761, 48.80952380952381]\n",
      "9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-7edb184924d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mearly_stopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train and test\n",
    "splitLength = len(update_report) // (num_cv + 1)\n",
    "slice_results = {}\n",
    "top_rank_k_accuracies = []\n",
    "# f1_measure = []\n",
    "for i in range(1, num_cv + 1):\n",
    "    print(i)\n",
    "    train_report = update_report[:i*splitLength-1]\n",
    "    train_owner = update_owner[:i*splitLength-1]\n",
    "    test_report = update_report[i*splitLength:(i+1)*splitLength-1]\n",
    "    test_owner = update_owner[i*splitLength:(i+1)*splitLength-1]\n",
    "        \n",
    "    # Remove data from test set that is not there in train set\n",
    "    train_owner_unique = set(train_owner)\n",
    "    test_owner_unique = set(test_owner)\n",
    "    unwanted_owner = list(test_owner_unique - train_owner_unique)\n",
    "    update_test_report = []\n",
    "    update_test_owner = []\n",
    "    for i in range(len(test_owner)):\n",
    "        if test_owner[i] not in unwanted_owner:\n",
    "            update_test_report.append(test_report[i])\n",
    "            update_test_owner.append(test_owner[i])\n",
    "    \n",
    "    unique_train_owner = list(set(train_owner))\n",
    "    classes = np.array(unique_train_owner)\n",
    "    \n",
    "    # Create train and test data\n",
    "    X_train = np.zeros(shape=[len(train_report), max_sentence_num, max_sentence_len], dtype=\"int32\")\n",
    "    Y_train = np.zeros(shape=[len(train_owner), 1], dtype=\"int32\")\n",
    "    for i, report in enumerate(train_report):\n",
    "        for j, sent in enumerate(report):\n",
    "            if j < max_sentence_num:\n",
    "                k = 0\n",
    "                for word in sent:\n",
    "                    if k < max_sentence_len:\n",
    "                        X_train[i, j, k] = word\n",
    "                        k = k + 1\n",
    "        Y_train[i, 0] = unique_train_owner.index(train_owner[i])\n",
    "    \n",
    "    X_test = np.zeros(shape=[len(update_test_report), max_sentence_num, max_sentence_len], dtype=\"int32\")\n",
    "    Y_test = np.zeros(shape=[len(update_test_owner), 1], dtype=\"int32\")\n",
    "    for i, report in enumerate(update_test_report):\n",
    "        for j, sent in enumerate(report):\n",
    "            if j < max_sentence_num:\n",
    "                k = 0\n",
    "                for word in sent:\n",
    "                    if k < max_sentence_len:\n",
    "                        X_test[i, j, k] = word\n",
    "                        k = k + 1\n",
    "        Y_test[i, 0] = unique_train_owner.index(update_test_owner[i])    \n",
    "    \n",
    "    y_train = np_utils.to_categorical(Y_train, len(unique_train_owner))\n",
    "    y_test = np_utils.to_categorical(Y_test, len(unique_train_owner))\n",
    "    \n",
    "    # Model\n",
    "    word_input = Input(shape=(max_sentence_len,), dtype='float32')\n",
    "    embedded_sequences = Embedding(len(embedding_matrix), embed_size, weights=[embedding_matrix], input_length=max_sentence_len, trainable=True)(word_input)\n",
    "    l_gru = Bidirectional(GRU(num_rnn_unit, return_sequences=True, dropout=0.2))(embedded_sequences)\n",
    "    l_dense = TimeDistributed(Dense(num_dense_unit))(l_gru)\n",
    "    l_att = HierarchicalAttentionNetwork(max_sentence_num)(l_dense)\n",
    "    word_encoder = Model(word_input, l_att)\n",
    "    \n",
    "    sent_input = Input(shape=(max_sentence_num, max_sentence_len), dtype='float32')\n",
    "    sent_encoder = TimeDistributed(word_encoder)(sent_input)\n",
    "    l_gru_sent = Bidirectional(GRU(num_rnn_unit, return_sequences=True, dropout=0.2))(sent_encoder)\n",
    "    l_dense_sent = TimeDistributed(Dense(num_dense_unit))(l_gru_sent)\n",
    "    l_att_sent = HierarchicalAttentionNetwork(max_sentence_len)(l_dense_sent)\n",
    "    preds = Dense(len(classes), activation='softmax')(l_att_sent)\n",
    "    model = Model(sent_input, preds)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=Adam(lr=1e-4), metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3)\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=500, callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    prediction = model.predict(X_test)\n",
    "    accuracy = topk_accuracy(prediction, y_test, classes, rank_k=rank_k)\n",
    "#     f1 = f_measure(prediction, y_test, classes, mode='macro')\n",
    "    print(\"CV{0}, top1 - ... - top{1} accuracy: \".format(i, rank_k), accuracy)\n",
    "    \n",
    "    train_result = hist.history\n",
    "    train_result[\"test_topk_accuracies\"] = accuracy\n",
    "    slice_results[i + 1] = train_result\n",
    "    top_rank_k_accuracies.append(accuracy[-1])\n",
    "#     f1_measure.append(f1)\n",
    "    \n",
    "    del model\n",
    "    \n",
    "print(\"Top{0} accuracies for all CVs: {1}\".format(rank_k, top_rank_k_accuracies))\n",
    "print(\"Average top{0} accuracy: {1}\".format(rank_k, sum(top_rank_k_accuracies)/rank_k))\n",
    "# print(f1_measure)\n",
    "# print(np.mean(f1_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
